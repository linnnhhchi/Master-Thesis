# -*- coding: utf-8 -*-
"""LSTM training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sN42wuegGqk1KfmPLMZoUpYIv8uTm2-h
"""

import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from keras.models import Model, Sequential
from keras.layers import Input, LSTM, Conv1D, Dropout, Dense, Embedding, Flatten, RepeatVector, Concatenate
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam, SGD, RMSprop
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import TimeSeriesSplit
from keras import backend as K
import tensorflow as tf
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

random.seed(404)
tf.random.set_seed(404)

df = pd.read_csv('/content/feature_data.csv')
df["date"] = pd.to_datetime(df["date"], infer_datetime_format=True, errors='coerce')
df.set_index('date', inplace=True)

X = df.drop(columns=['charged_energy'])
y = df['charged_energy']

# Encode land_types
label_encoder = LabelEncoder()
X['land_types'] = label_encoder.fit_transform(X['land_types'].astype(str))
X_land = np.array(X['land_types'], dtype=np.int32).reshape(-1, 1)  # reshaped for model input
X = X.drop(columns=['land_types'])

# Split data
train_size = int(len(X) * 0.8)
val_size = int(len(X) * 0.1)
test_size = len(X) - train_size - val_size

X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]
X_land_train, X_land_val, X_land_test = X_land[:train_size], X_land[train_size:train_size + val_size], X_land[train_size + val_size:]

# Feature types
numeric_features = ['year', 'temperature', 'dewpoint', 'road_density', 'commercial_density',
                    'residential_density', 'recreation_density', 'highway_proximity',
                    'public_transport_proximity', 'evcs_proximity', 'center_proximity',
                    'parking_density', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos']
binary = ['day_of_week', 'weekend', 'holiday', 'covid', 'time_slot']

for col in binary:
    X[col] = X[col].astype(str)

# Preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', MinMaxScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), binary)])

X_train = preprocessor.fit_transform(X_train, y_train)
X_val = preprocessor.transform(X_val)
X_test = preprocessor.transform(X_test)

# Reshape for LSTM
X_train_seq = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_val_seq = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))
X_test_seq = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

X_train_seq = np.array(X_train_seq, dtype=np.float32)
X_val_seq = np.array(X_val_seq, dtype=np.float32)
X_test_seq = np.array(X_test_seq, dtype=np.float32)

y_train = np.array(y_train, dtype=np.float32)
y_val = np.array(y_val, dtype=np.float32)
y_test = np.array(y_test, dtype=np.float32)

# Hyperparameters
best_params = {
    'lstm_units': 224,
    'dropout': 0.2,
    'optimizer': 'rmsprop',
    'learning_rate': 0.0009,
    'two_layers': True}

lstm_units = best_params["lstm_units"]
dropout_rate = best_params["dropout"]
optimizer_name = best_params["optimizer"]
learning_rate = best_params["learning_rate"]
two_layers = best_params["two_layers"]

# Optimizer selection
if optimizer_name == "adam":
    optimizer = Adam(learning_rate=learning_rate)
elif optimizer_name == "sgd":
    optimizer = SGD(learning_rate=learning_rate)
else:
    optimizer = RMSprop(learning_rate=learning_rate)

# Model definition
seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]), name='seq_input')
land_input = Input(shape=(1,), dtype='int32', name='land_input')

embedding = Embedding(input_dim=np.max(X_land) + 1, output_dim=5)(land_input)
embedding_flat = Flatten()(embedding)
embedding_repeat = RepeatVector(X_train_seq.shape[1])(embedding_flat)

merged = Concatenate()([seq_input, embedding_repeat])

x = LSTM(lstm_units // 2, activation='relu', return_sequences=two_layers)(merged)
x = Dropout(dropout_rate)(x)

if two_layers:
    x = LSTM(lstm_units, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

output = Dense(1)(x)

model = Model(inputs=[seq_input, land_input], outputs=output)
model.compile(loss='mean_absolute_error', optimizer=optimizer)
model.summary()

history = model.fit([X_train_seq, X_land_train], y_train,
          validation_data=([X_val_seq, X_land_val], y_val),
          epochs=30, batch_size=64, verbose=0)

y_val_pred = model.predict([X_val_seq, X_land_val], batch_size=64)
y_test_pred = model.predict([X_test_seq, X_land_test], batch_size=64)

val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
val_mae = mean_absolute_error(y_val, y_val_pred)
val_r2 = r2_score(y_val, y_val_pred)

test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f'Spatial LSTM Validation:\n MAE: {val_mae:.4f} \n RMSE: {val_rmse:.4f} \n R-squared: {val_r2:.4f}')
print(f'Spatial LSTM Test:\n MAE: {test_mae:.4f} \n RMSE: {test_rmse:.4f} \n R-squared: {test_r2:.4f}')

test_dates = df.index[train_size + val_size:]
compare = pd.DataFrame({
    'charged_energy': y_test.flatten(),
    'predicted': y_test_pred.flatten()}, index=test_dates)

# Group by day
compare['day'] = compare.index.normalize()
group = compare.groupby('day')[['charged_energy', 'predicted']].sum().reset_index()
group['mae'] = np.abs(group['charged_energy'] - group['predicted'])

fig, axs = plt.subplots(2, 1, figsize=(12, 6), layout='constrained')

# Actual vs Predicted
axs[0].plot(group['day'], group['charged_energy'], label='Actual', color='blue')
axs[0].plot(group['day'], group['predicted'], label='Predicted', color='orange')
axs[0].set_title('SpatioTemporal LSTM: Actual vs Predicted')
axs[0].set_ylabel('Energy Consumption (kWh)')
axs[0].legend()
axs[0].grid(True)
axs[0].xaxis.set_major_locator(mdates.MonthLocator())
axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
axs[0].tick_params(labelbottom=False)

# Daily MAE
axs[1].plot(group['day'], group['mae'], label='MAE', color='purple')
axs[1].set_xlabel('Date')
axs[1].set_ylabel('Error Score')
axs[1].legend()
axs[1].grid(True)
axs[1].xaxis.set_major_locator(mdates.MonthLocator())
axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
axs[1].tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.savefig('spatiotemporal_lstm.png', dpi=300)
plt.show()

import seaborn as sns

group['day'] = pd.to_datetime(group['day'])
group['month'] = group['day'].dt.month
group = group[(group['month'] >= 6) & (group['month'] <= 10)]

plt.figure(figsize=(10, 6))
sns.histplot(group['mae'], bins=30, kde=True, edgecolor='black', alpha=0.7)
plt.title('Residuals Histogram with KDE (After June)')
plt.xlabel('Residuals (MAE)')
plt.ylabel('Frequency')
plt.grid(True)
plt.savefig('spatial_lstm_residuals.png', dpi=300)
plt.show()

land_labels = label_encoder.inverse_transform(X_land_test.flatten())

test_data = pd.DataFrame({
    'charged_energy': y_test.flatten(),
    'predicted': y_test_pred.flatten(),
    'land_types': land_labels
}, index=df.index[-len(y_test):])  # assuming y_test is the last slice

# Add hour column
test_data['hour'] = test_data.index.hour

# Filter specific land types
land_types_to_plot = ['commercial', 'construction']
filtered = test_data[test_data['land_types'].isin(land_types_to_plot)]

# Group by hour and land_types
group_hourly = (filtered.groupby(['hour', 'land_types'])[['charged_energy', 'predicted']]
                .sum().reset_index())
group_hourly['mae'] = np.abs(group_hourly['charged_energy'] - group_hourly['predicted'])

# Plotting
fig, axs = plt.subplots(2, 2, figsize=(12, 6), sharex=True)
for i, land in enumerate(land_types_to_plot):
    subset = group_hourly[group_hourly['land_types'] == land]

    axs[0, i].plot(subset['hour'], subset['charged_energy'], label='Actual', marker='o', color='blue')
    axs[0, i].plot(subset['hour'], subset['predicted'], label='Predicted', marker='x', color='orange')
    axs[0, i].set_title(f"{land.capitalize()}")
    axs[0, i].set_xlim(0, 24)
    axs[0, i].set_xticks(np.arange(0, 25, 1))
    if i == 0:
        axs[0, i].set_ylabel('Energy Consumption (kWh)')

    axs[0, i].legend()
    axs[0, i].grid(True)

    axs[1, i].plot(subset['hour'], subset['mae'], label='MAE', marker='o', color='purple')
    axs[1, i].set_xlabel('Hour')
    axs[1, i].set_xlim(0, 24)
    axs[1, i].set_xticks(np.arange(0, 25, 1))

    if i == 0:
        axs[1, i].set_ylabel('Error Score')
    axs[1, i].legend()
    axs[1, i].grid(True)

plt.tight_layout()
plt.savefig('spatiotemporal_lstm_by_hour_day.png', dpi=300)
plt.show()

def get_all_feature_names(preprocessor):
    feature_names = []
    for name, transformer, columns in preprocessor.transformers_:
        if hasattr(transformer, 'get_feature_names_out'):
            try:
                names = transformer.get_feature_names_out(columns)
            except TypeError:
                names = transformer.get_feature_names_out()
            feature_names.extend(names)
        else:
            feature_names.extend(columns)
    return feature_names

feature_names = get_all_feature_names(preprocessor)

def evaluate_model(model, X_seq, land_input, y_true):
    y_pred = model.predict([X_seq, land_input], batch_size=32, verbose=0)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return mae, rmse, r2

baseline_mae, baseline_rmse, baseline_r2 = evaluate_model(model, X_test_seq, X_land_test, y_test)

importances = []
for i in range(X_test_seq.shape[2]):
    X_test_permuted = X_test_seq.copy()
    for t in range(X_test_permuted.shape[1]):
        X_test_permuted[:, t, i] = np.random.permutation(X_test_permuted[:, t, i])

    permuted_mae = mean_absolute_error(y_test, model.predict([X_test_permuted, X_land_test], batch_size=32, verbose=0))
    importances.append(permuted_mae - baseline_mae)

importances = np.array(importances)
importances_percentage = importances / importances.sum() * 100

sorted_idx = np.argsort(importances_percentage)
top_10_idx = sorted_idx[-10:]

plt.figure(figsize=(10, 6))
plt.barh(np.array(feature_names)[top_10_idx], importances_percentage[top_10_idx], color='steelblue')
plt.xlabel("Permutation Importance (%)")
plt.title("Top 10 Important Features")
plt.tight_layout()
plt.savefig('feature_importance_lstm.png', dpi=300)
plt.show()

"""**LSTM Without Spatial Prediction**

"""

df = pd.read_csv('/content/feature_data.csv')
df["date"] = pd.to_datetime(df["date"], infer_datetime_format=True, errors='coerce')
df.set_index('date', inplace=True)

X = df.drop(columns=['charged_energy', 'road_density', 'highway_proximity',
       'commercial_density', 'residential_density',
       'public_transport_proximity', 'evcs_proximity', 'parking_density',
       'recreation_density', 'center_proximity', 'land_types'])
y = df['charged_energy']

# Split data
train_size = int(len(X) * 0.8)
val_size = int(len(X) * 0.1)
test_size = len(X) - train_size - val_size

X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]

# Feature types
numeric_features = ['temperature', 'dewpoint', 'year', 'hour_sin', 'hour_cos','month_sin', 'month_cos']
binary = ['day_of_week', 'weekend', 'holiday', 'covid', 'time_slot']

for col in binary:
    X[col] = X[col].astype(str)

preprocessor = ColumnTransformer(transformers=[
    ('num', MinMaxScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), binary)])

X_train = preprocessor.fit_transform(X_train, y_train)
X_val = preprocessor.transform(X_val)
X_test = preprocessor.transform(X_test)

# Reshape for LSTM
X_train_seq_without = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_val_seq_without = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))
X_test_seq_without = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

X_train_seq_without = np.array(X_train_seq, dtype=np.float32)
X_val_seq_without = np.array(X_val_seq, dtype=np.float32)
X_test_seq_without = np.array(X_test_seq, dtype=np.float32)

y_train_without = np.array(y_train, dtype=np.float32)
y_val_without = np.array(y_val, dtype=np.float32)
y_test_without = np.array(y_test, dtype=np.float32)

# Hyperparameters
best_params = {
    'lstm_units': 224,
    'dropout': 0.2,
    'optimizer': 'rmsprop',
    'learning_rate': 0.0009,
    'two_layers': True}

lstm_units = best_params["lstm_units"]
dropout_rate = best_params["dropout"]
optimizer_name = best_params["optimizer"]
learning_rate = best_params["learning_rate"]
two_layers = best_params["two_layers"]

# Optimizer selection
if optimizer_name == "adam":
    optimizer = Adam(learning_rate=learning_rate)
elif optimizer_name == "sgd":
    optimizer = SGD(learning_rate=learning_rate)
else:
    optimizer = RMSprop(learning_rate=learning_rate)

model = Sequential()
model.add(Input(shape=(X_train_seq_without.shape[1], X_train_seq_without.shape[2])))
model.add(LSTM(lstm_units//2, activation='relu', return_sequences=two_layers))
model.add(Dropout(dropout_rate))

if two_layers:
  model.add(LSTM(lstm_units, activation='relu'))
  model.add(Dropout(dropout_rate))

model.add(Dense(1))
model.compile(loss='mean_absolute_error', optimizer=optimizer)
model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(X_train_seq_without, y_train_without,
          validation_data=(X_val_seq_without, y_val_without),
          epochs=30, batch_size=64, verbose=0,
          callbacks=[early_stopping])

y_val_pred_without = model.predict(X_val_seq_without, batch_size=64)
y_test_pred_without = model.predict(X_test_seq_without, batch_size=64)

val_rmse_without = np.sqrt(mean_squared_error(y_val_without, y_val_pred_without))
val_mae_without = mean_absolute_error(y_val_without, y_val_pred_without)
val_r2_without = r2_score(y_val_without, y_val_pred_without)

test_rmse_without = np.sqrt(mean_squared_error(y_test_without, y_test_pred_without))
test_mae_without = mean_absolute_error(y_test_without, y_test_pred_without)
test_r2_without = r2_score(y_test_without, y_test_pred_without)

print(f'Temporal LSTM Validation:\n MAE: {val_mae_without:.4f} \n RMSE: {val_rmse_without:.4f} \n R-squared: {val_r2_without:.4f}')
print(f'Temporal LSTM Test:\n MAE: {test_mae_without:.4f} \n RMSE: {test_rmse_without:.4f} \n R-squared: {test_r2_without:.4f}')

dates = df.index[train_size + val_size:]
compare_without = pd.DataFrame({
    'charged_energy': y_test_without.flatten(),
    'predicted': y_test_pred_without.flatten()}, index=dates)

# Group by day
compare_without['day'] = compare_without.index.normalize()
group = compare_without.groupby('day')[['charged_energy', 'predicted']].sum().reset_index()
group['mae'] = np.abs(group['charged_energy'] - group['predicted'])

fig, axs = plt.subplots(2, 1, figsize=(12, 6), layout='constrained')

# Actual vs Predicted
axs[0].plot(group['day'], group['charged_energy'], label='Actual', color='blue')
axs[0].plot(group['day'], group['predicted'], label='Predicted', color='orange')
axs[0].set_title('Temporal LSTM: Actual vs Predicted')
axs[0].set_ylabel('Energy Consumption (kWh)')
axs[0].legend()
axs[0].grid(True)
axs[0].xaxis.set_major_locator(mdates.MonthLocator())
axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
axs[0].tick_params(labelbottom=False)

# Daily MAE
axs[1].plot(group['day'], group['mae'], label='MAE', color='purple')
axs[1].set_xlabel('Date')
axs[1].set_ylabel('Error Score')
axs[1].legend()
axs[1].grid(True)
axs[1].xaxis.set_major_locator(mdates.MonthLocator())
axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
axs[1].tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.savefig('temporal_lstm.png', dpi=300)
plt.show()

land_types = df['land_types'].iloc[-len(y_test_without):].values

test_data = pd.DataFrame({
    'charged_energy': y_test_without.flatten(),
    'predicted': y_test_pred_without.flatten(),
    'land_types': land_types
}, index=df.index[-len(y_test_without):])  # assuming y_test is the last slice

# Add hour column
test_data['hour'] = test_data.index.hour

# Filter specific land types
land_types_to_plot = ['commercial', 'construction']
filtered = test_data[test_data['land_types'].isin(land_types_to_plot)]

# Group by hour and land_types
group_hourly = (filtered.groupby(['hour', 'land_types'])[['charged_energy', 'predicted']]
                .sum().reset_index())
group_hourly['mae'] = np.abs(group_hourly['charged_energy'] - group_hourly['predicted'])

# Plotting
fig, axs = plt.subplots(2, 2, figsize=(12, 6), sharex=True)
for i, land in enumerate(land_types_to_plot):
    subset = group_hourly[group_hourly['land_types'] == land]

    axs[0, i].plot(subset['hour'], subset['charged_energy'], label='Actual', marker='o', color='blue')
    axs[0, i].plot(subset['hour'], subset['predicted'], label='Predicted', marker='x', color='orange')
    axs[0, i].set_title(f"{land.capitalize()}")
    axs[0, i].set_xlim(0, 24)
    axs[0, i].set_xticks(np.arange(0, 25, 1))
    if i == 0:
        axs[0, i].set_ylabel('Energy Consumption (kWh)')

    axs[0, i].legend()
    axs[0, i].grid(True)

    axs[1, i].plot(subset['hour'], subset['mae'], label='MAE', marker='o', color='purple')
    axs[1, i].set_xlabel('Hour')
    axs[1, i].set_xlim(0, 24)
    axs[1, i].set_xticks(np.arange(0, 25, 1))

    if i == 0:
        axs[1, i].set_ylabel('Error Score')
    axs[1, i].legend()
    axs[1, i].grid(True)

plt.tight_layout()
plt.savefig('temporal_lstm_by_hour_day.png', dpi=300)
plt.show()

import seaborn as sns

group['day'] = pd.to_datetime(group['day'])
group['month'] = group['day'].dt.month
high_peak = group[(group['month'] >= 6) & (group['month'] <= 10)]

plt.figure(figsize=(10, 6))
sns.histplot(high_peak['mae'], bins=30, kde=True, edgecolor='black', alpha=0.7)
plt.title('Residuals Histogram with KDE (After June)')
plt.xlabel('Residuals (MAE)')
plt.ylabel('Frequency')
plt.grid(True)
plt.savefig('temporal_lstm_residual.png', dpi=300)
plt.show()

from scipy.stats import ttest_rel

y_test_pred_without = y_test_pred_without.flatten()
y_test_pred = y_test_pred.flatten()

errors_no_spatial = [0.8388, 0.8319, 0.8315, 0.8309, 0.8363, 0.8286, 0.8302, 0.8323, 0.8336, 0.8361]
errors_with_spatial = [0.8453, 0.8418, 0.8584, 0.8376, 0.8468, 0.8377, 0.8382, 0.8353, 0.8398, 0.8363]

t_stat, p_value = ttest_rel(errors_with_spatial, errors_no_spatial, alternative='less')
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")

import seaborn as sns

df_errors = pd.DataFrame({
    'No Spatial': errors_no_spatial,
    'With Spatial': errors_with_spatial
})

df_melted = df_errors.melt(var_name='LSTM', value_name='Absolute Error')

plt.figure(figsize=(8, 5))
sns.boxplot(x='LSTM', y='Absolute Error', data=df_melted)
plt.title('Prediction Error Distribution: With vs Without Spatial Features')
plt.grid(True)
plt.tight_layout()
plt.savefig('error_boxplot_lstm.png', dpi=300)
plt.show()

"""**LSTM Multi-step Prediction**"""

df = pd.read_csv('/content/feature_data.csv')
df["date"] = pd.to_datetime(df["date"], infer_datetime_format=True, errors='coerce')
df.set_index('date', inplace=True)

X = df.drop(columns=['charged_energy'])
y = df['charged_energy']

# Encode land_types
label_encoder = LabelEncoder()
X['land_types'] = label_encoder.fit_transform(X['land_types'].astype(str))
X_land = np.array(X['land_types'], dtype=np.int32).reshape(-1, 1)
X = X.drop(columns=['land_types'])

# Feature type definitions
numeric_features = ['year', 'temperature', 'dewpoint', 'road_density', 'commercial_density',
                    'residential_density', 'recreation_density', 'highway_proximity',
                    'public_transport_proximity', 'evcs_proximity', 'center_proximity',
                    'parking_density', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos']
binary = ['day_of_week', 'weekend', 'holiday', 'covid', 'time_slot']
for col in binary:
    X[col] = X[col].astype(str)

preprocessor = ColumnTransformer(transformers=[
    ('num', MinMaxScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), binary)])

X_scaled = preprocessor.fit_transform(X)

forecast_horizon = 24
time_steps = 24

X_seq, y_seq, land_seq = [], [], []

for i in range(len(X_scaled) - time_steps - forecast_horizon + 1):
    X_seq.append(X_scaled[i:i + time_steps])
    y_seq.append(y.iloc[i + time_steps:i + time_steps + forecast_horizon].values)
    land_seq.append(X_land[i + time_steps - 1])

X_seq = np.array(X_seq, dtype=np.float32)
y_seq = np.array(y_seq, dtype=np.float32)
land_seq = np.array(land_seq, dtype=np.int32)

train_size = int(len(X_seq) * 0.8)
val_size = int(len(X_seq) * 0.1)

X_train_seq = X_seq[:train_size]
X_val_seq = X_seq[train_size:train_size + val_size]
X_test_seq = X_seq[train_size + val_size:]

y_train = y_seq[:train_size]
y_val = y_seq[train_size:train_size + val_size]
y_test = y_seq[train_size + val_size:]

X_land_train = land_seq[:train_size]
X_land_val = land_seq[train_size:train_size + val_size]
X_land_test = land_seq[train_size + val_size:]


best_params = {
    'lstm_units': 224,
    'dropout': 0.2,
    'optimizer': 'rmsprop',
    'learning_rate': 0.0009,
    'two_layers': True}

lstm_units = best_params["lstm_units"]
dropout_rate = best_params["dropout"]
optimizer_name = best_params["optimizer"]
learning_rate = best_params["learning_rate"]
two_layers = best_params["two_layers"]

if optimizer_name == "adam":
    optimizer = Adam(learning_rate=learning_rate)
elif optimizer_name == "sgd":
    optimizer = SGD(learning_rate=learning_rate)
else:
    optimizer = RMSprop(learning_rate=learning_rate)

seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]), name='seq_input')
land_input = Input(shape=(1,), dtype='int32', name='land_input')

embedding = Embedding(input_dim=np.max(X_land) + 1, output_dim=5)(land_input)
embedding_flat = Flatten()(embedding)
embedding_repeat = RepeatVector(X_train_seq.shape[1])(embedding_flat)

merged = Concatenate()([seq_input, embedding_repeat])

x = LSTM(lstm_units // 2, activation='relu', return_sequences=two_layers)(merged)
x = Dropout(dropout_rate)(x)

if two_layers:
    x = LSTM(lstm_units, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

output = Dense(forecast_horizon)(x)

model = Model(inputs=[seq_input, land_input], outputs=output)
model.compile(loss='mae', optimizer=optimizer)
model.summary()

history = model.fit([X_train_seq, X_land_train], y_train,
                    validation_data=([X_val_seq, X_land_val], y_val),
                    epochs=30, batch_size=64)

y_pred = model.predict([X_test_seq, X_land_test], batch_size=64)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f'LSTM Test:\n MAE: {mae:.4f} \n RMSE: {rmse:.4f} \n R-squared: {r2:.4f}')

y_pred_flat = y_pred.flatten()
y_test_flat = y_test.flatten()

base_times = df.index[train_size + val_size + time_steps : train_size + val_size + time_steps + len(y_pred)]
timestamps = []

for base in base_times:
    timestamps.extend([base + pd.Timedelta(hours=i) for i in range(forecast_horizon)])

timestamps = pd.to_datetime(timestamps)

min_len = min(len(y_pred_flat), len(y_test_flat), len(timestamps))
y_pred_flat = y_pred_flat[:min_len]
y_test_flat = y_test_flat[:min_len]
timestamps = timestamps[:min_len]

compare_df = pd.DataFrame({
    'charged_energy': y_test_flat,
    'predicted': y_pred_flat}, index=timestamps)

compare_df['day'] = compare_df.index.normalize()
daily_group = (compare_df.groupby('day')[['charged_energy', 'predicted']].sum() / 20).reset_index()
daily_group['mae'] = np.abs(daily_group['charged_energy'] - daily_group['predicted'])

fig, axs = plt.subplots(2, 1, figsize=(12, 6), layout='constrained')

axs[0].plot(daily_group['day'], daily_group['charged_energy'], label='Actual', color='blue')
axs[0].plot(daily_group['day'], daily_group['predicted'], label='Predicted', color='orange')
axs[0].set_title('LSTM Daily Prediction')
axs[0].set_ylabel('Charged Energy (kWh)')
axs[0].legend()
axs[0].grid(True)
axs[0].xaxis.set_major_locator(mdates.MonthLocator())
axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
axs[0].tick_params(labelbottom=False)

axs[1].plot(daily_group['day'], daily_group['mae'], label='MAE', color='purple')
axs[1].set_xlabel('Date')
axs[1].set_ylabel('Mean Absolute Error (kWh)')
axs[1].legend()
axs[1].grid(True)
axs[1].xaxis.set_major_locator(mdates.MonthLocator())
axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))

plt.tight_layout()
plt.savefig('daily_lstm.png', dpi=300)
plt.show()

daily_group['residual'] = np.abs(daily_group['charged_energy'] - daily_group['predicted'])
daily_group['day'] = pd.to_datetime(daily_group['day'])
daily_group['month'] = daily_group['day'].dt.month
high_peak = daily_group[daily_group['month'] >= 6]

plt.figure(figsize=(10, 6))
sns.histplot(high_peak['residual'], bins=30, kde=True, edgecolor='black', alpha=0.7)
plt.title('Histogram of Daily Residuals')
plt.xlabel('Residual (kWh)')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.savefig('daily_residual_histogram.png', dpi=300)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df['land_types'] = df['land_types'].str.lower()

train_size = int(len(df) * 0.8)
val_size = int(len(df) * 0.1)
test_data = df.iloc[train_size + val_size:]

min_len = min(len(y_test_flat), len(y_pred_flat), len(test_data))
y_test_flat = y_test_flat[:min_len]
y_pred_flat = y_pred_flat[:min_len]
test_data_trimmed = test_data.iloc[:min_len].copy()

results_df = pd.DataFrame({
    'True Energy': y_test_flat,
    'Predicted Energy': y_pred_flat,
    'land_types': test_data_trimmed['land_types'].values
}, index=test_data_trimmed.index)

results_df.index = pd.to_datetime(results_df.index)
results_df['hour'] = results_df.index.hour

selected_types = ['commercial', 'construction']
results_df = results_df[results_df['land_types'].isin(selected_types)]

grouped = results_df.groupby(['land_types', 'hour']).agg({
    'True Energy': 'sum',
    'Predicted Energy': 'sum'
}).reset_index()
grouped['MAE'] = np.abs(grouped['True Energy'] - grouped['Predicted Energy'])

fig, axs = plt.subplots(2, 2, figsize=(12, 6), sharex=True)

for i, land in enumerate(selected_types):
    subset = grouped[grouped['land_types'] == land]

    # Actual vs Predicted (top row)
    axs[0, i].plot(subset['hour'], subset['True Energy'], label='Actual', marker='o', color='blue')
    axs[0, i].plot(subset['hour'], subset['Predicted Energy'], label='Predicted', marker='x', color='orange')
    axs[0, i].set_title(f"{land.capitalize()}")
    axs[0, i].set_xlim(0, 23)
    axs[0, i].set_xticks(np.arange(0, 24, 1))
    if i == 0:
        axs[0, i].set_ylabel('Energy Consumption (kWh)')
    axs[0, i].legend()
    axs[0, i].grid(True)

    axs[1, i].plot(subset['hour'], subset['MAE'], label='MAE', marker='o', color='purple')
    axs[1, i].set_xlabel('Hour')
    axs[1, i].set_xlim(0, 23)
    axs[1, i].set_xticks(np.arange(0, 24, 1))
    if i == 0:
        axs[1, i].set_ylabel('Error Score')
    axs[1, i].legend()
    axs[1, i].grid(True)

plt.tight_layout()
plt.savefig('24h_pred_lstm_commercial_construction.png', dpi=300)
plt.show()